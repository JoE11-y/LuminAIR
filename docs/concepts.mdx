---
title: 'Concepts'
description: 'A deep dive into LuminAir.'
---

LuminAir provides compilers built on top of [Luminal](https://github.com/jafioti/luminal), 
an ML library primarily designed to simplify adding support for new backends. 
We leveraged Luminal to develop an AI framework that facilitates the easy addition of new zkVMs 
and the parallelization of numerous operations, whether for trace or proof generation. 
Let's understand how LuminAir works.

<Info>
Some sections of this resource will discuss Luminal's characteristics, while others will focus specifically on LuminAir. 
You can read more about Luminal [here](https://luminalai.com/docs/introduction).
</Info>

## Lazy computation

When you initiate a graph and design your model as shown below, 
everything remains static until you call the `execute()` method. 
Writing an expression like `a + b` does not perform any computation immediately. 
Instead, the operation is recorded in a directed acyclic computation graph for later execution. 
The computation occurs only when `graph.execute()` is called.

```rust
  // Setup graph and tensors
  let mut cx = Graph::new();
  let a = cx.new_tensor::<R1<3>>()
      .set(vec![1.0, 2.0, 3.0]);
  let b = cx.new_tensor::<R1<3>>()
      .set(vec![1.0, 2.0, 3.0]);

  // Actual operations
  let mut c = (a + b).retrieve();

  // Compile the graph to run on the CairoVM
  let _ = cx.compile(CairoCompiler::default(), &mut c);

  // Run graph
  cx.execute();
```

This approach allows many optimization to be performed at compile time, reducing complexity of the circuits.

## RISC-style architecture

By default, Luminal simplifies the process of adding new devices by requiring only 11 mandatory primitive operators 
for a new device to implement, enabling support for any models.

- **Unary Operators:** `Log2, Exp2, Sin, Sqrt, Recip`
- **Binary Operators:** `Add, Mul, Mod, LessThan`
- **Other Operators:** `SumReduce, MaxReduce, Contiguous`

These operators are enough to support transformers, convolutional networks, and more.

So each time we want to support a new zkVM, we only need to implement the 11 mandatory zk circuits based on the VM we are adding.


Another advantage of this RISC-style architecture is the decomposition of complex computational graph into multiple simple programs (nodes), 
where each node can be an independent zk circuit running on a different zkVM instance. 
**This allows generating simple and independent traces that can then be proved in parallel.**

However, not everything needs to be decomposed into primitive operators. 
It's up to you (the compiler developer) to create compilers that fit the needs of the zkVM you are supporting.

For example, if it's less resource-intensive for your zkVM to subtract two tensors directly rather than decomposing the operation into `Add` and `Mul` (of -1), 
you can implement a compiler that detects this pattern of nodes and replaces it with a `Subtract` circuit.

Compilers are entirely separate from Luminal, allowing them to be fully implemented by third-party crates. That's what we did with LuminAir.
In this sense, **LuminAir offers a collection of compilers specialized in Zero-Knowledge Machine Learning**.

## GraphTensors

`GraphTensors` are central to Luminal's core concept. 
They serve as references to nodes within the computation graph.

You can create a `GraphTensor` as follow.
```rust
let mut cx = Graph::new(); // Initialize a new graph
let a: GraphTensor<R1<3>> = cx.tensor(); // Create a new tensor node in the graph
```

Once you have `GraphTensors`, you can perform various linear algebra operations similarly 
to libraries like [PyTorch](https://pytorch.org/):
```rust
let b = a.exp().sqrt(); // Apply exponentiation and square root operations
let c = b + a; // Add tensors b and a
```
Note that the operation `b + a` does not consume anything, 
and both `a` and `b` remain available for later use. 
This efficiency is due to `GraphTensor` being a super lightweight tracking structure that implements the `Copy` trait. 
This design allows operations on `GraphTensor` to construct a computation graph without immediate execution, 
so no values are assigned to the `GraphTensor` at this stage.

Actual computations are postponed until `cx.execute()` is invoked, 
which enables the compiler to optimize the execution process effectively.

In summary, `GraphTensors` can be understood as pointers to a specific node within the graph,
along with metadata about the node's output, such as its shape.

## Shape tracking

When defining a `GraphTensor`, you might notice that its type includes a specific generic parameter:

```rust
let a: GraphTensor<R1<3>> = cx.tensor();
```

The `R1<3>` in `GraphTensor<R1<3>>` specifies the tensor's shape at the type level. 
Here's a breakdown of what it represents:
- `R1`: Indicates the rank of the tensor, which is the number of dimensions it has. 
In this case, `R1` stands for a rank-1 tensor, meaning it's a one-dimensional tensor.
- `<3>`: Specifies the size of the tensor along its single dimension. 
Here, `<3>` means the tensor has 3 elements.

So, `R1<3>` defines a one-dimensional tensor with 3 elementsâ€”a 3-dimensional vector.

<Info> 
  Note: `R1<N>` is a type alias for `(Const<N>,)`, where `Const<N>` is a compile-time constant representing the size. 
  This allows Rust to enforce tensor shapes during compilation. 
</Info>

By incorporating tensor shapes directly into their types, 
the compiler ensures that shapes are tracked at compile time. 
This design makes sure tensor operations are only performed on tensors with compatible 
shapes and avoid shape calculations at running time. As part of LuminAir, this simplifies circuit logic.
